# Script for Experiment L: Weighted Averaging Ensemble of Experiments G, H, I, J

"""
Extended Description:
This script creates a *weighted* average ensemble of the predictions generated by:
- exp_g_xgb_combinations_te.py (XGBoost, Full Data, TE) [Weight: 0.4]
- exp_h_lgbm_combinations_te.py (LGBM L1, Full Data, TE) [Weight: 0.2]
- exp_i_lgbm_l2_combinations_te.py (LGBM L2, Full Data, TE) [Weight: 0.2]
- exp_j_lgbm_l2_combinations_ce.py (LGBM L2, Full Data, CE) [Weight: 0.2]

It loads their OOF and test predictions, averages them, calculates the
ensemble's OOF RMSE score, saves the averaged predictions, generates a
submission file, and updates the experiments tracker.

Corresponds to experiment 'exp_l_ensemble_weighted_ghij.py' in experiments.csv.
"""

import sys
import os
from pathlib import Path
import numpy as np
import pandas as pd
import polars as pl # Needed for loading target data

# --- Add project root to sys.path ---
PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

try:
    from utils.load_csv import load_csv
    from utils.calculate_metric import calculate_metric
    from utils.generate_submission_file import generate_submission_file
    from utils.setup_logging import setup_logging
    from utils.seed_everything import seed_everything
except ImportError as e:
    print(f"Error importing utility functions: {e}")
    print("Ensure utils directory is at the project root and contains all required files.")
    sys.exit(1)

# --- Configuration ---
EXPERIMENT_NAME = "exp_l_ensemble_weighted_ghij.py"
SEED = 42 # Keep consistent seeding although less critical here
TARGET_COL = "Listening_Time_minutes"

# Base experiment names (Ensure these scripts have completed successfully)
BASE_EXPERIMENTS = [
    "exp_g_xgb_combinations_te",
    "exp_h_lgbm_combinations_te",
    "exp_i_lgbm_l2_combinations_te",
    "exp_j_lgbm_l2_combinations_ce"
]
# Weights corresponding to BASE_EXPERIMENTS (must sum to 1.0)
WEIGHTS = [0.4, 0.2, 0.2, 0.2]
assert abs(sum(WEIGHTS) - 1.0) < 1e-6, "Weights must sum to 1.0"

# Paths
SCRIPT_DIR = Path(__file__).resolve().parent
COMPETITION_DIR = SCRIPT_DIR.parent
RAW_DATA_DIR = COMPETITION_DIR / "data" / "raw"
OOF_PREDS_DIR = COMPETITION_DIR / "predictions" / "oof"
TEST_PREDS_DIR = COMPETITION_DIR / "predictions" / "test"
EXPERIMENTS_CSV = COMPETITION_DIR / "experiments.csv"

# Input Data Paths (Raw data needed for target and IDs)
TRAIN_CSV = RAW_DATA_DIR / "train.csv"
TEST_CSV = RAW_DATA_DIR / "test.csv"

# Output Paths for this experiment
OOF_PRED_OUTPUT_PATH = OOF_PREDS_DIR / f"{EXPERIMENT_NAME.replace('.py', '')}_oof.npy"
TEST_PRED_OUTPUT_PATH = TEST_PREDS_DIR / f"{EXPERIMENT_NAME.replace('.py', '')}_test_avg.npy"
SUBMISSION_OUTPUT_PATH = TEST_PREDS_DIR / f"{EXPERIMENT_NAME.replace('.py', '')}_submission.csv"

# Create output directories if they don't exist
OOF_PREDS_DIR.mkdir(parents=True, exist_ok=True)
TEST_PREDS_DIR.mkdir(parents=True, exist_ok=True)

# --- Main Function ---
def main():
    logger = setup_logging(log_file=str(COMPETITION_DIR / "training.log"))
    seed_everything(SEED)
    logger.info(f"--- Starting Experiment: {EXPERIMENT_NAME} ---")

    oof_preds_list = []
    test_preds_list = []
    expected_oof_shape = None
    expected_test_shape = None

    # --- Load Predictions from Base Experiments ---
    logger.info(f"Loading predictions from base experiments: {BASE_EXPERIMENTS}")
    for base_exp in BASE_EXPERIMENTS:
        oof_path = OOF_PREDS_DIR / f"{base_exp}_oof.npy"
        test_path = TEST_PREDS_DIR / f"{base_exp}_test_avg.npy"

        try:
            logger.info(f"Loading OOF from: {oof_path}")
            oof = np.load(oof_path)
            oof_preds_list.append(oof)
            if expected_oof_shape is None:
                expected_oof_shape = oof.shape
            elif oof.shape != expected_oof_shape:
                logger.error(f"OOF shape mismatch for {base_exp}. Expected {expected_oof_shape}, got {oof.shape}")
                sys.exit(1)

            logger.info(f"Loading Test from: {test_path}")
            test = np.load(test_path)
            test_preds_list.append(test)
            if expected_test_shape is None:
                expected_test_shape = test.shape
            elif test.shape != expected_test_shape:
                logger.error(f"Test shape mismatch for {base_exp}. Expected {expected_test_shape}, got {test.shape}")
                sys.exit(1)

        except FileNotFoundError as e:
            logger.error(f"Prediction file not found for {base_exp}: {e}")
            logger.error("Ensure all base experiments (G, H, I, J) have run successfully.")
            sys.exit(1)
        except Exception as e:
            logger.error(f"Error loading predictions for {base_exp}: {e}")
            sys.exit(1)

    if len(oof_preds_list) != len(BASE_EXPERIMENTS) or len(test_preds_list) != len(BASE_EXPERIMENTS):
        logger.error("Failed to load predictions for all base experiments.")
        sys.exit(1)

    # --- Load Target Data (and clean NaNs as done in base experiments) ---
    logger.info("Loading and cleaning target data for OOF validation...")
    try:
        df_train_orig = load_csv(str(TRAIN_CSV))
        target_pd_full = df_train_orig[TARGET_COL].to_pandas()
        
        # Replicate NaN dropping from base scripts
        nan_target_mask = target_pd_full.isna()
        if nan_target_mask.any():
            target_pd_clean = target_pd_full[~nan_target_mask].reset_index(drop=True)
            logger.info(f"Loaded original target and removed {nan_target_mask.sum()} NaN values. Clean target shape: {target_pd_clean.shape}")
            # Sanity check against loaded OOF shape
            if target_pd_clean.shape[0] != expected_oof_shape[0]:
                 logger.warning(f"Cleaned target shape {target_pd_clean.shape} does not match OOF shape {expected_oof_shape}. Check base experiment NaN handling.")
                 # Proceeding, but score might be invalid if shapes mismatch
        else:
            target_pd_clean = target_pd_full
            logger.info("Loaded original target. No NaN values found.")

    except Exception as e:
        logger.error(f"Error loading or cleaning target data: {e}")
        sys.exit(1)

    # --- Load Test IDs ---
    try:
        df_test_orig = load_csv(str(TEST_CSV))
        test_ids = df_test_orig['id'].to_pandas()
        if test_ids.shape[0] != expected_test_shape[0]:
             logger.error(f"Test ID shape {test_ids.shape} does not match Test Prediction shape {expected_test_shape}. Check test data loading.")
             sys.exit(1)
        logger.info(f"Loaded {len(test_ids)} test IDs.")
    except Exception as e:
        logger.error(f"Error loading test IDs: {e}")
        sys.exit(1)
        
    # --- Weighted Average Predictions ---
    logger.info(f"Calculating weighted average of OOF and Test predictions with weights: {WEIGHTS}...")
    avg_oof_preds = np.average(np.array(oof_preds_list), axis=0, weights=WEIGHTS)
    avg_test_preds = np.average(np.array(test_preds_list), axis=0, weights=WEIGHTS)
    logger.info(f"Weighted Averaged OOF shape: {avg_oof_preds.shape}, Weighted Averaged Test shape: {avg_test_preds.shape}")

    # --- Calculate OOF Score ---
    try:
        # Ensure y_true is writeable by making a copy
        overall_cv_score = calculate_metric(y_true=target_pd_clean.copy(), y_pred=avg_oof_preds, metric_name='rmse')
        logger.info(f"Ensemble Overall CV Score (RMSE): {overall_cv_score:.5f}")
    except Exception as e:
        logger.error(f"Error calculating ensemble CV score: {e}")
        overall_cv_score = np.nan # Set score to NaN if calculation fails

    # --- Save Averaged Predictions ---
    logger.info("Saving averaged OOF and Test predictions...")
    try:
        np.save(OOF_PRED_OUTPUT_PATH, avg_oof_preds)
        logger.info(f"Averaged OOF predictions saved to: {OOF_PRED_OUTPUT_PATH}")
        np.save(TEST_PRED_OUTPUT_PATH, avg_test_preds)
        logger.info(f"Averaged Test predictions saved to: {TEST_PRED_OUTPUT_PATH}")
    except Exception as e:
        logger.error(f"Error saving averaged predictions: {e}")
        sys.exit(1)

    # --- Generate Submission File ---
    logger.info("Generating submission file...")
    try:
        generate_submission_file(
            ids=test_ids, # Already loaded as pandas Series
            predictions=avg_test_preds,
            id_col_name='id',
            target_col_name=TARGET_COL,
            file_path=str(SUBMISSION_OUTPUT_PATH)
        )
        logger.info(f"Submission file saved to: {SUBMISSION_OUTPUT_PATH}")
    except Exception as e:
        logger.error(f"Error generating submission file: {e}")
        sys.exit(1)

    # --- Update Experiments Tracker ---
    try:
        logger.info(f"Updating experiments tracker: {EXPERIMENTS_CSV}")
        experiments_df = pd.read_csv(EXPERIMENTS_CSV, sep=';')
        exp_mask = experiments_df['script_file_name'] == EXPERIMENT_NAME
        
        # Prepare description for the tracker
        base_exp_names_str = ", ".join([exp.split('_')[1] for exp in BASE_EXPERIMENTS]) # e.g., g, h, i, j
        weights_str = ", ".join(map(str, WEIGHTS))
        why_desc = f"Weighted Ensemble ({weights_str}) of Exp {base_exp_names_str}"
        what_desc = f"Weighted avg OOF/Test predictions from Exp {base_exp_names_str} (Weights: {weights_str})"
        how_desc = f"Load .npy files from {base_exp_names_str}, calculate np.average with weights {weights_str}, save results."

        if exp_mask.sum() > 0:
            logger.warning(f"Experiment '{EXPERIMENT_NAME}' already exists in tracker. Updating existing entry.")
            experiments_df.loc[exp_mask, 'why'] = why_desc
            experiments_df.loc[exp_mask, 'what'] = what_desc
            experiments_df.loc[exp_mask, 'how'] = how_desc
            experiments_df.loc[exp_mask, 'cv_score'] = overall_cv_score
            experiments_df.loc[exp_mask, 'status'] = 'done'
            logger.info(f"Updated existing experiment '{EXPERIMENT_NAME}' with CV score: {overall_cv_score:.5f} and status: done")
        else:
             logger.info(f"Adding new experiment '{EXPERIMENT_NAME}' to tracker.")
             new_row = pd.DataFrame([{
                 'script_file_name': EXPERIMENT_NAME,
                 'why': why_desc,
                 'what': what_desc,
                 'how': how_desc,
                 'cv_score': overall_cv_score,
                 'lb_score': None, # LB score needs manual update
                 'status': 'done'
             }])
             # Ensure columns align
             for col in experiments_df.columns:
                  if col not in new_row.columns:
                      new_row[col] = None
             new_row = new_row[experiments_df.columns]
             experiments_df = pd.concat([experiments_df, new_row], ignore_index=True)

        experiments_df.to_csv(EXPERIMENTS_CSV, index=False, sep=';')
        logger.info("Experiments tracker updated successfully.")
    except Exception as e:
        logger.error(f"An error occurred while updating experiments tracker: {e}")

    logger.info(f"--- Experiment Finished: {EXPERIMENT_NAME} ---")

if __name__ == "__main__":
    main() 