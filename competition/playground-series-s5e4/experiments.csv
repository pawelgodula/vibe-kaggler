script_file_name;why;what;how;cv_score;public_lb;private_lb;status;comment
exp_b_train_lgbm_single_fold.py;Establish a very basic baseline performance.;Train LightGBM on a single fold of the data.;Use train_single_fold.py utility with a default LGBM configuration. No special feature engineering.;13.137812765028572;13.27;pending;done;Baseline LGBM.
exp_c_train_lgbm_cv.py;Get a more robust performance estimate using cross-validation.;Train LightGBM using all CV folds.;Use train_and_cv.py utility with default LGBM configuration.;13.16222605641724;13.26;pending;done;Baseline LGBM with CV.
exp_d_train_lgbm_cv_catencode.py;Assess the impact of basic categorical feature handling.;Train LightGBM using CV folds with basic categorical encoding.;Modify 03_c script/config to include encode_categorical_features.py (e.g., OneHotEncoding) before training.;13.10198675310286;13.18;pending;done;Basic categorical encoding improves CV and LB slightly.
exp_e_train_xgb_cv.py;Evaluate an alternative gradient boosting model.;Train XGBoost using all CV folds.;Use train_and_cv.py utility with default XGBoost configuration.;13.068041190459647;13.13;pending;done;Baseline XGBoost - slightly better than LGBM baseline.
exp_f_ensemble_avg.py;Check if a simple ensemble improves over individual models.;Create an average ensemble of multi-fold LGBM and XGBoost predictions.;Use average_predictions.py utility on the OOF/test predictions from experiments C and E.;13.08820766717235;13.17;pending;done;Simple avg ensemble of C & E improves CV but not LB.
exp_g_xgb_combinations_te.py;Rerun Exp G (XGB+Combinations+TE) on FULL data;Train XGBoost with extensive FE including N-way label encoded combinations and TE on full data.;Load ext data, clip, map, poly/agg/N-way(label) feats, OOF TE, KFold(7) XGB training.;12.467021947126003;pending;pending;done;XGB with advanced FE (Combinations + TE). Strong CV.
exp_h_lgbm_combinations_te.py;Adapt Exp G (XGB) to LGBM with full data;Train LGBM with extensive FE including N-way label encoded combinations and TE;Load ext data, clip, map, poly/agg/N-way(label) feats, OOF TE, KFold(7) LGBM training.;13.5208702009429;12.017;pending;done;**Best single model on Public LB**. LGBM version of Exp G. CV score is worse than XGB, but LB is much better. Good candidate for future comps.
exp_i_lgbm_l2_combinations_te.py;Test L2 objective in LGBM based on Exp H;Train LGBM (L2 objective) with extensive FE including N-way label encoded combinations and TE;Same as Exp H but objective=regression_l2.;12.712032925682802;pending;pending;done;LGBM L2 objective. Improves CV over Exp H, LB TBD.
exp_j_lgbm_l2_combinations_ce.py;Replace TE with CE in Exp I (LGBM L2);Train LGBM (L2 objective) with extensive FE including N-way label encoded combinations and Count Encoding;Same as Exp I but apply_count_encoding used instead of apply_target_encoding.;12.708546183471322;pending;pending;done;LGBM L2 with Count Encoding instead of TE. Similar CV to Exp I, LB TBD.
exp_k_ensemble_avg_ghij.py;Average Ensemble of Exp g, h, i, j;Average OOF/Test predictions from Exp g, h, i, j;Load .npy files from g, h, i, j, calculate np.mean, save results.;12.42046135370567;;pending;done;Average ensemble of advanced models (G, H, I, J). Good CV.
exp_l_ensemble_weighted_ghij.py;Weighted Ensemble (0.4, 0.2, 0.2, 0.2) of Exp g, h, i, j;Weighted avg OOF/Test predictions from Exp g, h, i, j (Weights: 0.4, 0.2, 0.2, 0.2);Load .npy files from g, h, i, j, calculate np.average with weights 0.4, 0.2, 0.2, 0.2, save results.;12.387274578389533;12.003;11.921;done;**Best ensemble and overall best score on Public and Private LB**. Weighted average of G, H, I, J.
