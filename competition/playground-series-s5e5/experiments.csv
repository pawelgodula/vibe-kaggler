script_file_name;why;what;how;cv_score;lb_score;status;experiment_type;base_experiment;new_feature
exp_a_lgbm_combinations_te.py;Adapt script for S5E5, 10% data, log1p target transform.;Train LGBM on log1p(Calories) with L1/RMSE metric.;Load 10%, log1p target, FE adapted, KFold(7) LGBM (L1/RMSE on log), expm1 preds, final CV RMSLE.;0.0658876756355329;0.06165;done;single_model;;Baseline with 10% data, combinations and target encoding
exp_b_lgbm_combinations_te.py;Adapt script for S5E5, full data, log1p target transform.;Train LGBM on log1p(Calories) with L1/RMSE metric.;Load 100% data, log1p target, FE adapted, KFold(7) LGBM (L1/RMSE on log), expm1 preds, final CV RMSLE.;0.0613135535307751;0.0587;done;single_model;exp_a_lgbm_combinations_te.py;Full dataset (100% vs 10%)
exp_c_lgbm_normalized_target.py;Normalize target by Duration to predict calories per minute.;Train LGBM on log1p(Calories/Duration), transform back for evaluation.;Full data, normalize target by Duration, apply log1p, LGBM, transform back: expm1(pred)*Duration.;0.0610298187307052;0.05843;done;single_model;exp_b_lgbm_combinations_te.py;Normalized target (Calories/Duration)
exp_d_ensemble_avg_abc.py;Ensemble the predictions from exp_a, exp_b, and exp_c to improve performance.;Simple average ensemble of three LGBM approaches.;Average OOF/test predictions from exp_a (10%), exp_b (100%), and exp_c (normalized target).;0.0606296521680213;0.05861;done;ensemble;exp_a,exp_b,exp_c;Average ensemble
exp_e_ensemble_bc.py;Ensemble only the predictions from exp_b and exp_c (both using 100% data).;Simple average ensemble of two full-data LGBM approaches.;Average OOF/test predictions from exp_b (100%, log1p) and exp_c (100%, normalized target).;0.0606296521680213;;done;ensemble;exp_b,exp_c;Average ensemble of full data models
exp_f_lgbm_enhanced_interactions.py;Add enhanced interaction features (multiply, divide, power) to exp_b.;Train LGBM with additional feature engineering using enhanced interactions.;Load 100% data, add division and power operations to feature set, log1p target, KFold(7) LGBM, expm1 preds, final CV RMSLE.;0.0154590650236574;1.11;done;single_model;exp_b_lgbm_combinations_te.py;Enhanced interaction features with target leakage
exp_h_lgbm_no_leakage.py;Fix target leakage in exp_f by removing Calories-based features.;Train LGBM with enhanced interaction features without using target variable.;Remove Calories/Duration and Calories/Weight interaction features, full data, log1p target, LGBM, KFold(7), expm1 preds.;0.0613126712967831;;done;single_model;exp_f_lgbm_enhanced_interactions.py;Fix target leakage
exp_i_lgbm_count_encoding.py;Test count encoding and categorical treatment of numerical features.;Train LGBM using count encoding on categorical features, including discretized numerical features.;Convert numeric features to categorical bins, apply count encoding, KFold(7) LGBM, log1p target, expm1 preds.;0.0608305901643903;0.05869;done;single_model;exp_b_lgbm_combinations_te.py;Count encoding + numerical features as categorical
exp_j_lgbm_count_encoding_test.py;Test count encoding based on test set frequencies and visualize feature importance.;Train LGBM using count encoding on categorical features with test set as encoding source.;Convert numeric features to categorical bins, apply count encoding using TEST data, KFold(7) LGBM, generate feature importance.;0.0607878351414485;;done;single_model;exp_i_lgbm_count_encoding.py;Count encoding from test data + feature importance visualization
exp_k_lgbm_normalized_target.py;Test target normalization by duration to predict calories per minute.;Train LGBM using count encoding and target normalization by duration.;Normalize target by dividing Calories by Duration, apply count encoding using TEST data, KFold(7) LGBM, generate feature importance.;0.0606794688015922;;done;single_model;exp_j_lgbm_count_encoding_test.py;Target normalization by duration (calories per minute)
exp_l_lgbm_ratio_encoding.py;Test ratio-to-category mean encoding with normalized target.;Train LGBM using count encoding, ratio-to-category mean encoding, and target normalization.;Normalize target by dividing Calories by Duration, apply count encoding and ratio-to-category mean encoding, KFold(7) LGBM, generate feature importance.;0.0606751441371511;;done;single_model;exp_k_lgbm_normalized_target.py;Ratio-to-category mean encoding
exp_m_clipped_predictions.py;Evaluate impact of clipping predictions to [1, 250].;Load existing predictions, clip to specified range, recalculate CV score.;Loaded OOF/test predictions from exp_l_lgbm_ratio_encoding.py, applied np.clip(1, 250), recalculated RMSLE.;0.0607629023015024;;done;post_processing;exp_l_lgbm_ratio_encoding.py;Prediction clipping [1, 250]
exp_n_soft_clipping.py;Evaluate impact of soft clipping with threshold=250.;Load existing predictions, apply logarithmic transformation to high values, recalculate CV score.;Loaded OOF/test predictions from exp_l_lgbm_ratio_encoding.py, applied threshold + log1p(value - threshold) for values > 250, recalculated RMSLE.;0.0607363557447466;;done;post_processing;exp_l_lgbm_ratio_encoding.py;Soft clipping with threshold=250
exp_o_lgbm_tuned_params.py;Test tuned LightGBM parameters without target normalization.;Train LightGBM with specific parameters (max_depth=10, learning_rate=0.02, etc.) using count encoding.;Apply count encoding using TEST data, train LightGBM with tuned parameters, KFold(7), log1p target (no normalization).;0.061349994860943;;done;single_model;exp_k_lgbm_normalized_target.py;Tuned LightGBM parameters
exp_p_lgbm_cross_terms.py;"Implement the LightGBM model from ""onlycatboost-score0-05684"" notebook.";Train LightGBM with cross-term features between numerical variables.;Create cross-terms, handle Sex as categorical, 5-fold CV, log1p transform target.;0.0600818359114473;;done;single_model;notebook: onlycatboost-score0-05684;Cross-term features
exp_q_lgbm_cross_terms_normalized.py;Combine cross-term features with target normalization.;Train LightGBM with cross-term features using normalized target (calories per minute).;Create cross-terms, normalize target by Duration, 5-fold CV, transform predictions back: expm1(pred)*Duration.;0.0598418565511814;;done;single_model;exp_p_lgbm_cross_terms.py;Target normalization (calories per minute)
exp_r_multi_model_cross_terms_normalized.py;Compare multiple models with the best feature engineering approach.;Train LightGBM, XGBoost and CatBoost with cross-term features and normalized target.;Create cross-terms, normalize target by Duration, train 3 model types, best model: CatBoost.;0.0594546647446643;;done;multi_model;exp_q_lgbm_cross_terms_normalized.py;Multiple model comparison (best: CatBoost)
exp_s_catboost_target_encoding.py;Enhance CatBoost model with target encoding for categorical features.;Train CatBoost with cross-term features, target normalization, and target encoding.;Apply target encoding to Sex and categorical versions of numeric features, keep cross-terms and target normalization by Duration.;0.0595391501300157;;done;single_model;exp_r_multi_model_cross_terms_normalized.py;Target encoding for categorical features
exp_t_catboost_no_normalization.py;Evaluate CatBoost performance without target normalization.;Train CatBoost with cross-term features but without target normalization.;Create cross-terms, use direct log1p(Calories) target without normalization by Duration.;0.0595654922024036;;done;single_model;exp_r_multi_model_cross_terms_normalized.py;No target normalization (direct log1p)
exp_u_catboost_n_way_multiplication.py;Test the effectiveness of higher-order feature multiplications (beyond pairs).;Train CatBoost with n-way multiplication features (up to 5-way) and normalized target.;Create n-way multiplication features up to 5-way (2-way: 15, 3-way: 20, 4-way: 15, 5-way: 6), normalize target by Duration.;0.05958;;done;single_model;exp_r_multi_model_cross_terms_normalized.py;N-way multiplication (up to 5-way)
exp_v_multi_model_direct_log_target.py;Replicate external script with reported better scores.;Train LGBM, XGB, CatBoost with direct log1p(Calories) target, cross-terms.;Direct log1p target, specific model params/fit calls, best model: XGBoost.;0.0599198653306829;;done;multi_model;exp_r_multi_model_cross_terms_normalized.py & external script;Direct log target, simplified LGBM fit, CatBoost default params (iter,lr,depth)
exp_w01_xgb_more_estimators.py;Increase n_estimators for XGBoost in exp_v setup.;Train XGBoost with n_estimators=5000, lr=0.01. Other models from exp_v.;XGB n_est=5000,lr=0.01. Direct log1p target. Best: XGBoost.;0.0598815444978435;;done;multi_model;exp_v_multi_model_direct_log_target.py;XGBoost: n_estimators=5000, lr=0.01
exp_x_updated_imports_check.py;Verify CV after refactoring: add_cross_terms, plot_fi, save_artifacts, update_tracker to utils.;Run exp_x logic with major components moved to utility functions.;exp_x refactored. Core modeling loop remains. Best model: XGBoost.;0.05991986533068294;;done;multi_model_refactored;exp_x_updated_imports_check.py;Major refactor of helper functions into utils.
