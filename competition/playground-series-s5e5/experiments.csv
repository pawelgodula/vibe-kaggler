script_file_name;why;what;how;cv_score;lb_score;status;experiment_type;base_experiment;new_feature
exp_a_lgbm_combinations_te.py;Adapt script for S5E5, 10% data, log1p target transform.;Train LGBM on log1p(Calories) with L1/RMSE metric.;Load 10%, log1p target, FE adapted, KFold(7) LGBM (L1/RMSE on log), expm1 preds, final CV RMSLE.;0.0658876756355329;0.06165;done;single_model;;Baseline with 10% data, combinations and target encoding
exp_b_lgbm_combinations_te.py;Adapt script for S5E5, full data, log1p target transform.;Train LGBM on log1p(Calories) with L1/RMSE metric.;Load 100% data, log1p target, FE adapted, KFold(7) LGBM (L1/RMSE on log), expm1 preds, final CV RMSLE.;0.0613135535307751;0.0587;done;single_model;exp_a_lgbm_combinations_te.py;Full dataset (100% vs 10%)
exp_c_lgbm_normalized_target.py;Normalize target by Duration to predict calories per minute.;Train LGBM on log1p(Calories/Duration), transform back for evaluation.;Full data, normalize target by Duration, apply log1p, LGBM, transform back: expm1(pred)*Duration.;0.0610298187307052;0.05843;done;single_model;exp_b_lgbm_combinations_te.py;Normalized target (Calories/Duration)
exp_d_ensemble_avg_abc.py;Ensemble the predictions from exp_a, exp_b, and exp_c to improve performance.;Simple average ensemble of three LGBM approaches.;Average OOF/test predictions from exp_a (10%), exp_b (100%), and exp_c (normalized target).;0.0606296521680213;0.05861;done;ensemble;exp_a,exp_b,exp_c;Average ensemble
exp_e_ensemble_bc.py;Ensemble only the predictions from exp_b and exp_c (both using 100% data).;Simple average ensemble of two full-data LGBM approaches.;Average OOF/test predictions from exp_b (100%, log1p) and exp_c (100%, normalized target).;0.0606296521680213;;done;ensemble;exp_b,exp_c;Average ensemble of full data models
exp_f_lgbm_enhanced_interactions.py;Add enhanced interaction features (multiply, divide, power) to exp_b.;Train LGBM with additional feature engineering using enhanced interactions.;Load 100% data, add division and power operations to feature set, log1p target, KFold(7) LGBM, expm1 preds, final CV RMSLE.;0.0154590650236574;1.11;done;single_model;exp_b_lgbm_combinations_te.py;Enhanced interaction features with target leakage
exp_h_lgbm_no_leakage.py;Fix target leakage in exp_f by removing Calories-based features.;Train LGBM with enhanced interaction features without using target variable.;Remove Calories/Duration and Calories/Weight interaction features, full data, log1p target, LGBM, KFold(7), expm1 preds.;0.0613126712967831;;done;single_model;exp_f_lgbm_enhanced_interactions.py;Fix target leakage
exp_i_lgbm_count_encoding.py;Test count encoding and categorical treatment of numerical features.;Train LGBM using count encoding on categorical features, including discretized numerical features.;Convert numeric features to categorical bins, apply count encoding, KFold(7) LGBM, log1p target, expm1 preds.;0.0608305901643903;0.05869;done;single_model;exp_b_lgbm_combinations_te.py;Count encoding + numerical features as categorical
exp_j_lgbm_count_encoding_test.py;Test count encoding based on test set frequencies and visualize feature importance.;Train LGBM using count encoding on categorical features with test set as encoding source.;Convert numeric features to categorical bins, apply count encoding using TEST data, KFold(7) LGBM, generate feature importance.;0.0607878351414485;;done;single_model;exp_i_lgbm_count_encoding.py;Count encoding from test data + feature importance visualization
exp_k_lgbm_normalized_target.py;Test target normalization by duration to predict calories per minute.;Train LGBM using count encoding and target normalization by duration.;Normalize target by dividing Calories by Duration, apply count encoding using TEST data, KFold(7) LGBM, generate feature importance.;0.0606794688015922;;done;single_model;exp_j_lgbm_count_encoding_test.py;Target normalization by duration (calories per minute)
exp_l_lgbm_ratio_encoding.py;Test ratio-to-category mean encoding with normalized target.;Train LGBM using count encoding, ratio-to-category mean encoding, and target normalization.;Normalize target by dividing Calories by Duration, apply count encoding and ratio-to-category mean encoding, KFold(7) LGBM, generate feature importance.;0.0606751441371511;;done;single_model;exp_k_lgbm_normalized_target.py;Ratio-to-category mean encoding
exp_m_clipped_predictions.py;Evaluate impact of clipping predictions to [1, 250].;Load existing predictions, clip to specified range, recalculate CV score.;Loaded OOF/test predictions from exp_l_lgbm_ratio_encoding.py, applied np.clip(1, 250), recalculated RMSLE.;0.0607629023015024;;done;post_processing;exp_l_lgbm_ratio_encoding.py;Prediction clipping [1, 250]
exp_n_soft_clipping.py;Evaluate impact of soft clipping with threshold=250.;Load existing predictions, apply logarithmic transformation to high values, recalculate CV score.;Loaded OOF/test predictions from exp_l_lgbm_ratio_encoding.py, applied threshold + log1p(value - threshold) for values > 250, recalculated RMSLE.;0.0607363557447466;;done;post_processing;exp_l_lgbm_ratio_encoding.py;Soft clipping with threshold=250
exp_o_lgbm_tuned_params.py;Test tuned LightGBM parameters without target normalization.;Train LightGBM with specific parameters (max_depth=10, learning_rate=0.02, etc.) using count encoding.;Apply count encoding using TEST data, train LightGBM with tuned parameters, KFold(7), log1p target (no normalization).;0.061349994860943;;done;single_model;exp_k_lgbm_normalized_target.py;Tuned LightGBM parameters
exp_p_lgbm_cross_terms.py;"Implement the LightGBM model from ""onlycatboost-score0-05684"" notebook.";Train LightGBM with cross-term features between numerical variables.;Create cross-terms, handle Sex as categorical, 5-fold CV, log1p transform target.;0.0600818359114473;;done;single_model;notebook: onlycatboost-score0-05684;Cross-term features
exp_q_lgbm_cross_terms_normalized.py;Combine cross-term features with target normalization.;Train LightGBM with cross-term features using normalized target (calories per minute).;Create cross-terms, normalize target by Duration, 5-fold CV, transform predictions back: expm1(pred)*Duration.;0.0598418565511814;;done;single_model;exp_p_lgbm_cross_terms.py;Target normalization (calories per minute)
exp_r_multi_model_cross_terms_normalized.py;Compare multiple models with the best feature engineering approach.;Train LightGBM, XGBoost and CatBoost with cross-term features and normalized target.;Create cross-terms, normalize target by Duration, train 3 model types, best model: CatBoost.;0.0594546647446643;;done;multi_model;exp_q_lgbm_cross_terms_normalized.py;Multiple model comparison (best: CatBoost)
exp_s_catboost_target_encoding.py;Enhance CatBoost model with target encoding for categorical features.;Train CatBoost with cross-term features, target normalization, and target encoding.;Apply target encoding to Sex and categorical versions of numeric features, keep cross-terms and target normalization by Duration.;0.0595391501300157;;done;single_model;exp_r_multi_model_cross_terms_normalized.py;Target encoding for categorical features
exp_t_catboost_no_normalization.py;Evaluate CatBoost performance without target normalization.;Train CatBoost with cross-term features but without target normalization.;Create cross-terms, use direct log1p(Calories) target without normalization by Duration.;0.0595654922024036;;done;single_model;exp_r_multi_model_cross_terms_normalized.py;No target normalization (direct log1p)
exp_u_catboost_n_way_multiplication.py;Test the effectiveness of higher-order feature multiplications (beyond pairs).;Train CatBoost with n-way multiplication features (up to 5-way) and normalized target.;Create n-way multiplication features up to 5-way (2-way: 15, 3-way: 20, 4-way: 15, 5-way: 6), normalize target by Duration.;0.05958;;done;single_model;exp_r_multi_model_cross_terms_normalized.py;N-way multiplication (up to 5-way)
exp_v_multi_model_direct_log_target.py;Replicate external script with reported better scores.;Train LGBM, XGB, CatBoost with direct log1p(Calories) target, cross-terms.;Direct log1p target, specific model params/fit calls, best model: XGBoost.;0.0599198653306829;;done;multi_model;exp_r_multi_model_cross_terms_normalized.py & external script;Direct log target, simplified LGBM fit, CatBoost default params (iter,lr,depth)
exp_w01_xgb_more_estimators.py;Increase n_estimators for XGBoost in exp_v setup.;Train XGBoost with n_estimators=5000, lr=0.01. Other models from exp_v.;XGB n_est=5000,lr=0.01. Direct log1p target. Best: XGBoost.;0.05988154449784351;;done;multi_model;exp_v_multi_model_direct_log_target.py;XGBoost: n_estimators=5000, lr=0.01
exp_w02_catboost_tuned_direct_log.py;Use tuned CatBoost params (from exp_r) with direct log target.;Train CatBoost with iter=2000, lr=0.02, depth=10. Other models from exp_v.;CATBOOST_PARAMS: iter=2000, lr=0.02, depth=10. Direct log1p target.;;;planned;multi_model;exp_v_multi_model_direct_log_target.py;CatBoost: iter=2000, lr=0.02, depth=10 (direct log)
exp_w03_lgbm_tuned_direct_log.py;Use tuned LGBM params & fit (from exp_r) with direct log target.;Train LGBM with exp_r params and fit call. Other models from exp_v.;LGBM_PARAMS from exp_r, eval_set/callbacks. Direct log1p target.;;;planned;multi_model;exp_v_multi_model_direct_log_target.py;LGBM: tuned params from exp_r (direct log)
exp_w04_all_tuned_direct_log.py;Combine tuned params for all models from w01-w03.;Train all models with their tuned versions from w01-w03. Direct log1p target.;Combined tuned params. Direct log1p target.;;;planned;multi_model;exp_v_multi_model_direct_log_target.py;All models tuned (direct log)
exp_w05_xgb_normalized_target.py;Test XGBoost (exp_v params) with normalized target (exp_r style).;Train XGBoost with exp_v params on log1p(Calories/Duration) target.;XGB_PARAMS from exp_v. Target: log1p(Cal/Dur), un-norm preds with *Dur.;;;planned;multi_model;exp_v_multi_model_direct_log_target.py;XGBoost with normalized target
exp_w06_feature_selection_rfe_xgb_direct_log.py;Use RFE with XGBoost to select top 20 features, then retrain exp_v models.;Perform RFE with XGB, select top 20 features. Retrain LGBM,XGB,CB from exp_v.;RFE(XGB, n_features_to_select=20). Direct log1p target.;;;planned;multi_model;exp_v_multi_model_direct_log_target.py;RFE feature selection (top 20 for XGB)
exp_w07_n_way_mult_3_direct_log.py;Test 2-way and 3-way multiplications with direct log target.;Use n_way_multiplication util for max_n_way=3. Models from exp_v.;max_n_way=3 for feature engineering. Direct log1p target.;;;planned;multi_model;exp_v_multi_model_direct_log_target.py;2-way & 3-way multiplications
exp_w08_no_body_temp_features_direct_log.py;Remove Body_Temp and its cross-terms. Models from exp_v.;Modify NUMERICAL_FEATURES to exclude Body_Temp. Direct log1p target.;Exclude Body_Temp from cross-terms. Direct log1p target.;;;planned;multi_model;exp_v_multi_model_direct_log_target.py;No Body_Temp features
exp_w09_catboost_more_tuning_direct_log.py;Further tune CatBoost (from w02) with l2_leaf_reg, border_count.;Train CatBoost (w02 base) with l2_leaf_reg=5, border_count=64.;CATBOOST_PARAMS: iter,lr,depth from w02 + l2_leaf_reg=5, border_count=64. Direct log1p target.;;;planned;multi_model;exp_w02_catboost_tuned_direct_log.py;CatBoost: l2_leaf_reg=5, border_count=64
exp_w10_simple_blend_top2_exp_v_exp_r.py;Simple average blend of best preds from exp_v (XGB) and exp_r (CatBoost).;Load OOF/test preds from exp_v XGB and exp_r CatBoost, average them.;Average 0.5*XGB(exp_v) + 0.5*CatBoost(exp_r).;;;planned;ensemble;exp_v, exp_r;Blend exp_v XGB & exp_r CatBoost
